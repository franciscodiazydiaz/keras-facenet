{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imageio import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_basepath = '../data/images-out/'\n",
    "names = [dir.split('/')[-1] for dir in glob.glob(os.path.join(image_dir_basepath, '*'))]\n",
    "\n",
    "# AWS\n",
    "region = 'us-east-1'\n",
    "bucket_name = 'fdiaz-dataset-07feb2020-01'\n",
    "collection_id = 'fdiaz-poc'\n",
    "\n",
    "# Destroy resources once we get a result\n",
    "destroy = False\n",
    "upload_to_s3 = False\n",
    "\n",
    "facenet_error_ids = [10, 33, 39, 62, 84, 87, 99, 111, 118, 137, 183, 194, 195]\n",
    "\n",
    "boto3.setup_default_session(profile_name='fdiaz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "class Rekognition(object):\n",
    "    def __init__(self, collection_id, region_name='us-east-1'):\n",
    "        \"\"\"\n",
    "        param: collection_id: Name of the Rekognition Collection.\n",
    "        param: region_name: AWS region name.\n",
    "        \"\"\"\n",
    "        self.client = boto3.client('rekognition', region_name=region_name)\n",
    "        self.collection_id = collection_id\n",
    "        self.face_ids = {}\n",
    "        \n",
    "    def list_faces(self, max_results=10):\n",
    "        return self.client.list_faces(CollectionId=self.collection_id,\n",
    "                                      MaxResults=max_results)\n",
    "\n",
    "    def index_face(self, name, bucket, image):\n",
    "        response = self.client.index_faces(CollectionId=self.collection_id,\n",
    "                                           Image={'S3Object': {'Bucket': bucket,'Name': image}},\n",
    "                                           MaxFaces=1,\n",
    "                                           QualityFilter=\"AUTO\",\n",
    "                                           DetectionAttributes=['ALL'])\n",
    "\n",
    "        for record in response['FaceRecords']:\n",
    "            face_id = record['Face']['FaceId']\n",
    "            if face_id in self.face_ids.keys():\n",
    "                print('Image already in collection: {} ({})'.format(name, face_id))\n",
    "            else:\n",
    "                self.face_ids[face_id] = {'name': name,\n",
    "                                          'image': image}\n",
    "                print('Index image for: {} ({})'.format(name, face_id))\n",
    "    \n",
    "        return response\n",
    "\n",
    "    def train(self, bucket, images, set_name='train'):\n",
    "        response = []\n",
    "\n",
    "        for image in images:\n",
    "            name, filename = self.__s3_path(image, set_name)\n",
    "            r = self.index_face(name, bucket, filename)\n",
    "            response.append(r)\n",
    "\n",
    "        return response\n",
    "        \n",
    "    def infer(self, bucket, images, set_name='test', max_faces=1, threshold=50):\n",
    "        response = []\n",
    "        pred = []\n",
    "        proba = []\n",
    "        \n",
    "        for image in images:\n",
    "            _, filename = self.__s3_path(image, set_name)\n",
    "            print('Processing: {}'.format(filename))\n",
    "            \n",
    "            result = client.search_faces_by_image(CollectionId=self.collection_id,\n",
    "                                                  Image={'S3Object': {'Bucket': bucket,'Name': filename}},\n",
    "                                                  FaceMatchThreshold=threshold,\n",
    "                                                  MaxFaces=max_faces)\n",
    "            response.append(result)\n",
    "            \n",
    "            if len(result['FaceMatches']) > 0:\n",
    "                face = result['FaceMatches'][0]\n",
    "                pred.append(self.face_ids[face['Face']['FaceId']])\n",
    "                proba.append(face['Similarity'])\n",
    "\n",
    "        return pred, proba, response\n",
    "    \n",
    "    def __s3_path(self, image, set_name):\n",
    "        img = os.path.basename(image)\n",
    "        name = image.split('/')[-2]\n",
    "        filename = os.path.join(set_name, name, img)\n",
    "        return name, filename\n",
    "    \n",
    "    def save(self, filename='faceids.sav'):\n",
    "        joblib.dump(self.face_ids, filename)\n",
    "\n",
    "    def load(self, filename='faceids.sav'):\n",
    "        if os.path.isfile(filename):\n",
    "            self.face_ids = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate AWS account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the account where we will execute the \"training\" and inference is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()\n",
    "account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "train_size = None\n",
    "dataset_x = []\n",
    "dataset_y = []\n",
    "\n",
    "for name in names:\n",
    "    dirpath = os.path.abspath(os.path.join(image_dir_basepath, name))\n",
    "    image_paths = glob.glob(os.path.join(dirpath, '*.jpg'))\n",
    "    dataset_x.extend(image_paths)\n",
    "    dataset_y.extend([name] * len(image_paths))\n",
    "    \n",
    "print('Dataset size: {}'.format(len(dataset_x)))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset_x, dataset_y, random_state = 0,\n",
    "                                                    train_size = train_size, test_size = test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.create_bucket(Bucket=bucket_name,\n",
    "                 ACL='private')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload images to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket_name, set_name, images):\n",
    "    \"\"\"\n",
    "    param: bucket_name: S3 bucket name.\n",
    "    param: set_name: Dataset name usually test, train, validation.\n",
    "    param: images: List of local image paths.\n",
    "    \"\"\"\n",
    "    for image in images:\n",
    "    img = os.path.basename(image)\n",
    "    name = image.split('/')[-2]\n",
    "    filename = os.path.join(set_name, name, img)\n",
    "    print('Uploading: {}'.format(filename))\n",
    "    s3.Object(bucket_name, filename,\n",
    "              ExtraArgs={'ServerSideEncryption': 'AES256'}).upload_file(image)   \n",
    "\n",
    "if upload_to_s3:\n",
    "    upload_to_s3(bucket_name, 'train', x_train)\n",
    "    upload_to_s3(bucket_name, 'test', x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('rekognition', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_collection(CollectionId=collection_id)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rk = Rekognition(collection_id)\n",
    "rk.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List faces should return an empty list as the Collection is new. Once we start populating the collection with faces we will get a non empty list. We can also validate the FaceModelVersion our Collection is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rk.list_faces()\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = rk.train(bucket_name, x_train)\n",
    "train_records[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, proba, infer_response = rk.infer(bucket_name, x_test, max_faces=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procRk = []\n",
    "train = []\n",
    "\n",
    "for i in rk.face_ids.values():\n",
    "    procRk.append(i['image'].split('/')[-1])\n",
    "    \n",
    "for i in x_train:\n",
    "    train.append(i.split('/')[-1])\n",
    "    \n",
    "(train[:2], procRk[:2])\n",
    "(len(train), len(procRk))\n",
    "for i, e in enumerate(train):\n",
    "    if e not in procRk:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = [p['name'] for p in pred]\n",
    "\n",
    "print('Test accuracy: {:.3f}'.format(metrics.accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = len(x_test)\n",
    "img_per_row = 8\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(faces/img_per_row), img_per_row, figsize=(20, 60), sharex='col', sharey='row')\n",
    "fig.subplots_adjust(hspace=0.4)#, wspace=0.1)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for row in range(math.ceil(faces/img_per_row)):\n",
    "    for col in range(8):\n",
    "        if count < len(y_pred):\n",
    "            if y_pred[count] == y_test[count]:\n",
    "                box = {'facecolor':'none', 'edgecolor':'none'}\n",
    "            else:\n",
    "                box = {'facecolor':'yellow', 'alpha':0.2}\n",
    "            axes[row, col].set_title('{} ({:.2f})'.format(y_pred[count], proba[count]), bbox=box)\n",
    "            axes[row, col].imshow(imread(x_test[count]))\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])\n",
    "        count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(proba)\n",
    "'Mean: {:.2f}, Std: {:.2f}, Min: {:.2f}, Max: {:.2f}, Total: {}'.format(p.mean(), p.std(), p.min(), p.max(), len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot([proba], labels=['Rekognition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destroy S3 bucket and Rekognition Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "for key in bucket.objects.all():\n",
    "    key.delete()\n",
    "bucket.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
